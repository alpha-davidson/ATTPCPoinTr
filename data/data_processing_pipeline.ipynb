{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c11461c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c53f5ea",
   "metadata": {},
   "source": [
    "# User Input Desired Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1851c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change file directory to the .h5 data file to convert\n",
    "file = h5py.File('data/output_digi_HDF_Mg22_Ne20pp_8MeV.h5', 'r')\n",
    "\n",
    "original_keys = list(file.keys())\n",
    "original_length = len(original_keys)\n",
    "# print(original_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29922e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6642\n"
     ]
    }
   ],
   "source": [
    "#making an array of the lengths of events\n",
    "event_lens = np.zeros(original_length, int)\n",
    "for i in range(original_length):\n",
    "    event = original_keys[i]\n",
    "    event_lens[i] = len(file[event])\n",
    "\n",
    "discards = 0\n",
    "for i in event_lens:\n",
    "    if i < 128:\n",
    "        discards += 1\n",
    "\n",
    "print(discards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0336a93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVENT CLASSIFICATION\n",
      "BINARY\n",
      "[0, 1, 2, 4, 5, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "sample_size = 128 #enter the size to which events will be up/downsampled\n",
    "\n",
    "# designed for data in the following format:\n",
    "# x[0] ,y[1] ,z[2] ,time[3], Amplitude[4], trackID (particle ID)[5], pointID[6]\n",
    "# energy[7] ,energy loss[8] ,angle[9], Mass[10], Atomic number[11], Event_id index[12], number of tracks[13]\n",
    "# ^ this is thus designed for simulated data, as much of this will not be known for experimental data.\n",
    "\n",
    "# Enter in the CLASSIFICATION and PROJECTIONS to evaluate in all caps as projection:\n",
    "# B is BINARY, T is tertiary, and FOUR is four track classiification\n",
    "# Set TRACK_CLASS to true if performing track classification (i.e. using scannet) and to false if performing event\n",
    "# classification (i.e. using modelnet)\n",
    "TRACK_CLASS = False\n",
    "class_type = 'BINARY' # type of track classification\n",
    "PROJECTION = 'XYZQ' # the three dimensions which will be input into the model\n",
    "ISOTOPE = 'Mg22'\n",
    "PROJ_TO_COLS = [0,1,2,4,5,12,13]\n",
    "\n",
    "PROJ_TO_COLS = {'XYZQ' : [0,1,2,4,5,12,13], 'XYZ': [0,1,2,5,12,13], 'XYQ': [0,1,4,5,12,13], 'XZQ': [0,2,4,5,12,13], 'YZQ': [1,2,4,5,12,13]}\n",
    "\n",
    "user_input = PROJ_TO_COLS[PROJECTION]\n",
    "print('TRACK CLASSIFICATION' if TRACK_CLASS else 'EVENT CLASSIFICATION')\n",
    "print(class_type)\n",
    "print(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bd442f",
   "metadata": {},
   "source": [
    "# Convert Raw H5 File into npArray with Corresponding key index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e22b9137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#making a numpy array of the data. three dimension are [event number, point within event, data value at point]\n",
    "#length of each event is based on the longest event in dataset, so non-maximal events are padded with zeros at the end\n",
    "#12th index of each data point now corresponds to the index of the event in the h5 file's original_keys\n",
    "# each point thus contains:\n",
    "# x,y,z, time, Amplitude, trackID (particle ID), pointID, energy, energy loss, angle, Mass, Atomic number, Event_id index\n",
    "file_name = ISOTOPE + '_w_key_index'\n",
    "# **only doing this if the file doens't exist already, as the conversion takes a while**\n",
    "if not os.path.exists('data/' + file_name + '.npy'):\n",
    "    event_data = np.zeros((original_length, np.max(event_lens), 13), float) \n",
    "    for n in range(len(original_keys)):\n",
    "        name = original_keys[n]\n",
    "        event = file[name]\n",
    "        ev_len = len(event)\n",
    "        #converting event into an array\n",
    "        for i,e in enumerate(event):\n",
    "            instant = np.array(list(e))\n",
    "            event_data[n][i][:12] = np.array(instant)\n",
    "            event_data[n][i][-1] = float(n) #insert index value to find corresponding event ID\n",
    "    np.save('data/' + file_name, event_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c027ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Assertion Statements to Check the Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14923148",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load( 'data/' + ISOTOPE + '_w_key_index' + '.npy')\n",
    "assert data.shape == (original_length, np.max(event_lens), 13), 'Array has incorrect shape'\n",
    "assert len(np.unique(data[:,:,12])) == original_length, 'Array has incorrect Event_ids'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3a0723",
   "metadata": {},
   "source": [
    "# Random sample From New Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60a50369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128, 14)\n"
     ]
    }
   ],
   "source": [
    "#adding 13th column to correspond to the number of tracks in event, zero-indexed: 0 = beam, 1= two track, 2 = 3 track...\n",
    "data_array = ISOTOPE + '_w_key_index.npy' #insert desired array to sample from \n",
    "new_array_name = ISOTOPE + '_size' + str(sample_size) + '_sampled'\n",
    "data = np.load('data/' + data_array)\n",
    "new_data = np.zeros((original_length, sample_size, 14), float)\n",
    "count = 0\n",
    "for i in range(original_length):\n",
    "    ev_len = event_lens[i]    #length of event-- i.e. number of points\n",
    "    particle_ids = data[i][:ev_len,5]\n",
    "    label, distr = np.unique(particle_ids, return_counts=True)\n",
    "    shortest = label[np.argmin(distr)]\n",
    "    shortest_ind = np.argwhere(particle_ids == shortest)\n",
    "    if ev_len == sample_size:    #if array is already preferred length\n",
    "        new_data[i][:,:-1] = data[i][:ev_len,:]\n",
    "\n",
    "    else:\n",
    "        instant = 0\n",
    "        #the first instances sampled will be those belonging to the shortest track to prevent it from being lost\n",
    "        for n in range(shortest_ind.size):\n",
    "            new_data[i,instant,:-1] = data[i,shortest_ind[n],:]\n",
    "            instant += 1\n",
    "        #     assert instant < 128, 'If instant >= 128 IndexError will occur'\n",
    "        need = sample_size #- shortest_ind.size\n",
    "        random_points = np.random.choice(range(ev_len), need, replace= True if need > ev_len else False)  #choosing random instances to sample\n",
    "        for r in random_points:\n",
    "            new_data[i,instant,:-1] = data[i,r,:] \n",
    "            instant += 1\n",
    "        # assert instant <= 128, 'If instant >= 128 IndexError will occur'\n",
    "    unique_point_ids = np.unique(data[i,:ev_len,5])    #array of unique particle IDs\n",
    "    new_data[i][0][-1] = unique_point_ids.size - 1    #number of unique particles, scaled to start at 0\n",
    "# np.save('data/' + new_array_name, new_data) \n",
    "print(new_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c92d4b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3358, 128, 14)\n"
     ]
    }
   ],
   "source": [
    "idxs = []\n",
    "for i in range(len(new_data)):\n",
    "\n",
    "    if new_data[i][:, :-1].any() == np.zeros((128, 13)).all():\n",
    "        idxs.append(i)\n",
    "\n",
    "idxs = np.asarray(idxs)\n",
    "\n",
    "# print(len(idxs))\n",
    "# print(count)\n",
    "assert len(idxs) == count\n",
    "\n",
    "cut_new_data = np.ndarray((len(new_data)-len(idxs), 128, 14))\n",
    "\n",
    "inc = 0\n",
    "\n",
    "for i in range(len(new_data)):\n",
    "\n",
    "    if i not in idxs:\n",
    "        cut_new_data[inc] = new_data[i]\n",
    "        inc += 1\n",
    "\n",
    "\n",
    "np.save('data/' + new_array_name, cut_new_data) \n",
    "print(cut_new_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a67eb23",
   "metadata": {},
   "source": [
    "### Assertion Statements to Check the Data After Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0bc290a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Array has incorrect number of tracks",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload( \u001b[39m'\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m ISOTOPE \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_size\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(sample_size) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_sampled.npy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[39massert\u001b[39;00m data\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (original_length\u001b[39m-\u001b[39mcount, sample_size, \u001b[39m14\u001b[39m), \u001b[39m'\u001b[39m\u001b[39mArray has incorrect shape -- \u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(data\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 3\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(data[:,:,\u001b[39m13\u001b[39m])) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(data[:,:,\u001b[39m5\u001b[39m]))\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mArray has incorrect number of tracks\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Array has incorrect number of tracks"
     ]
    }
   ],
   "source": [
    "data = np.load( 'data/' + ISOTOPE + '_size' + str(sample_size) + '_sampled.npy')\n",
    "assert data.shape == (original_length-count, sample_size, 14), 'Array has incorrect shape -- '+str(data.shape)\n",
    "assert len(np.unique(data[:,:,13])) == len(np.unique(data[:,:,5]))-1, 'Array has incorrect number of tracks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8aed41",
   "metadata": {},
   "source": [
    "### Check Distribution of labels after sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3829ba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5] [0 1 2 3 4 5]\n",
      "[6643   73  948 2315   16    5]\n",
      "[6652  156 1226 1951   12    3]\n",
      "Events changed = 370\n"
     ]
    }
   ],
   "source": [
    "#cheking how the distribution of labels changes from sampling\n",
    "name = ISOTOPE + '_size' + str(sample_size) + '_sampled'\n",
    "data = np.load('data/' + name + '.npy')\n",
    "real_tracks = np.zeros(original_length,int) \n",
    "sampled_tracks = np.zeros(original_length,int)\n",
    "\n",
    "for i in range(original_length-count):\n",
    "    ev_nt = data[i]\n",
    "    real_tracks[i] = ev_nt[0,-1]\n",
    "    unique_point_ids = np.unique(ev_nt[:,5])    #array of unqiue particles IDs\n",
    "    sampled_tracks[i] = unique_point_ids.size - 1\n",
    "    \n",
    "og_label, og_distr = np.unique(real_tracks, return_counts=True)\n",
    "new_label, new_distr = np.unique(sampled_tracks, return_counts=True)\n",
    "print(og_label, new_label)\n",
    "print(og_distr)\n",
    "print(new_distr)\n",
    "# number of events that have lost a track from sampling. not a big deal if nonzero\n",
    "print('Events changed = ' + str(np.sum(np.abs(new_distr - og_distr))//2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94310f47",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create Filtered npArrays\n",
    "For track classification, focusing on 4-track events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcee89c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert TRACK_CLASS, 'only do this for track classification'\n",
    "file_name = ISOTOPE + '_size' + str(sample_size) + '_sampled' #insert desired file name to open\n",
    "raw_data = np.load('data/' + file_name + '.npy')\n",
    "new_file_name = ISOTOPE + '_4-track_size' + str(sample_size)\n",
    "count = 0\n",
    "new_data = np.zeros((original_length, sample_size, 14), float)\n",
    "\n",
    "for i in range(original_length):\n",
    "    new_event = raw_data[i]\n",
    "    unique_point_ids = np.unique(new_event[:,5])    \n",
    "    current_tracks = unique_point_ids.size - 1 \n",
    "    og_tracks = new_event[0,-1]\n",
    "    \n",
    "    #omitting non-4-track events, mislabeled events, and that one event with a particle ID 4\n",
    "    if og_tracks != 3 or og_tracks != current_tracks or 5 in unique_point_ids:\n",
    "        continue\n",
    "    else:\n",
    "        new_event[:,5] -= 1 # lowering particle id to start from 0\n",
    "        new_data[count,:,:] = new_event\n",
    "        count += 1\n",
    "    \n",
    "print(count)\n",
    "saving = new_data[:count, :,:]\n",
    "np.save('data/' + new_file_name, saving) #creating new np array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d03e76b",
   "metadata": {},
   "source": [
    "### Assertion Statements to Check the Data After Filtering Tracks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59a2724",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRACK_CLASS:\n",
    "    data = np.load( 'data/' + ISOTOPE + '_4-track_size' + str(sample_size)+ '.npy')\n",
    "    print(list(np.unique(data[:,:,5])))\n",
    "    assert data.shape == (count, sample_size, 14), 'Array has incorrect shape'\n",
    "    assert list(np.unique(data[:,:,5])) == [0.0, 1.0, 2.0, 3.0], 'Array has incorrect particle ids'\n",
    "    assert len(np.unique(data[:,:,5])) == 4, 'Array has incorrect number of particle ids'\n",
    "    assert list(np.unique(saving[:,0,13])) == [3.0], 'Array has incorrect type of events'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac188a3",
   "metadata": {},
   "source": [
    "### Check Distribution of Event Types After Filtering Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdc26f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
      "[0.0, 1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "[   1   73  948 2315   16    5]\n",
      "[  10  156 1226 1951   12    3]\n",
      "[1. 2. 3. 4. 5. 6.]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Event labels do not match actual event types",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m unique_point_id \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(data[:,:,\u001b[39m5\u001b[39m])\n\u001b[1;32m     23\u001b[0m \u001b[39mprint\u001b[39m((unique_point_id)) \u001b[39m# uncomment to check particle ids\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[39massert\u001b[39;00m (np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mabs(new_distr \u001b[39m-\u001b[39m og_distr))) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mEvent labels do not match actual event types\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Event labels do not match actual event types"
     ]
    }
   ],
   "source": [
    "if TRACK_CLASS:\n",
    "    name = ISOTOPE + '_4-track_size' + str(sample_size)\n",
    "else:\n",
    "    name = ISOTOPE + '_size' + str(sample_size) + '_sampled'\n",
    "data = np.load('data/' + name + '.npy')\n",
    "real_tracks = np.zeros(len(data),int)\n",
    "sampled_tracks = np.zeros(len(data),int)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    ev_nt = data[i]\n",
    "    real_tracks[i] = ev_nt[0,-1] # original event label\n",
    "    unique_point_ids = np.unique(ev_nt[:,5])    # array of unqiue particles IDs-- i.e current event label after sampling\n",
    "    sampled_tracks[i] = unique_point_ids.size - 1\n",
    "    \n",
    "label, og_distr = np.unique(real_tracks, return_counts=True)\n",
    "label, new_distr = np.unique(sampled_tracks, return_counts=True)\n",
    "\n",
    "print(list(np.unique(data[:,:,5]))) # Particles present in data\n",
    "print(list(np.unique(data[:,:,13]))) # types of events present in data\n",
    "print(og_distr) \n",
    "print(new_distr) \n",
    "unique_point_id = np.unique(data[:,:,5])\n",
    "print((unique_point_id)) # uncomment to check particle ids\n",
    "assert (np.sum(np.abs(new_distr - og_distr))) == 0, 'Event labels do not match actual event types'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152a94c8",
   "metadata": {},
   "source": [
    "# Binary, Tertiary, and Other Classification\n",
    "### *Skip this step if doing event classification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c76b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to adjust point labels based on track classification type, with particle 0 being the alpha\n",
    "# No need to do this step if doing event classification\n",
    "if TRACK_CLASS:\n",
    "    if class_type == 'BINARY':\n",
    "        def alpha(p):\n",
    "            if int(p) == 0:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "    elif class_type == 'TERTIARY':\n",
    "        def alpha(p):\n",
    "                if int(p) == 0:\n",
    "                    return 0\n",
    "                elif int(p) == 1:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a469a15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to do this step if doing event classification\n",
    "#adjusting particle IDs\n",
    "if TRACK_CLASS:\n",
    "    if class_type == 'BINARY' or class_type == 'TERTIARY':\n",
    "        name = ISOTOPE + '_4-track_size' + str(sample_size)\n",
    "        data = np.load('data/' + name + '.npy')\n",
    "        new_classification = np.zeros((len(data), sample_size, 14), float)\n",
    "        for i in tqdm.tqdm(range(len(data))):\n",
    "            event = data[i]\n",
    "            new_event = event[:,:]\n",
    "            new_event[:,5] = list(map(alpha, event[:,5]))\n",
    "            new_classification[i,:,:] = new_event[:,:]\n",
    "    np.save('data/' + name, new_classification)\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcce33d",
   "metadata": {},
   "source": [
    "# Split Testing Set, Training Set, and Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e9147d",
   "metadata": {},
   "source": [
    "## Split Training and Testing Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b92adb9",
   "metadata": {},
   "source": [
    "Performs a 20-test 20-val 60-train split on all 4-track events, and generates an array of numbers as long as the length of the data to randomize the events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e90af17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(671, 128, 14) (672, 128, 14) (2015, 128, 14)\n"
     ]
    }
   ],
   "source": [
    "if TRACK_CLASS:\n",
    "    name = ISOTOPE + '_4-track_size' + str(sample_size)\n",
    "    all_events = np.load('data/' + name + '.npy')\n",
    "else:\n",
    "    name = ISOTOPE + '_size' + str(sample_size)\n",
    "    all_events = np.load('data/' + name + '_sampled.npy')\n",
    "\n",
    "rand_shuffle = np.random.choice(len(all_events), len(all_events), replace = False)\n",
    "\n",
    "\n",
    "# 20-20 marking for test and validation\n",
    "test_split = int(len(all_events) * .2)\n",
    "val_split = int(len(all_events) * .4)\n",
    "\n",
    "if TRACK_CLASS:\n",
    "    test_event_indices =  all_events[rand_shuffle[:test_split],:,-2:]    #only saving the indices and number of tracks of the test events\n",
    "else:\n",
    "    test_data = all_events[rand_shuffle[:test_split],:,:]\n",
    "val_data = all_events[rand_shuffle[test_split:val_split],:,:]\n",
    "train_data = all_events[rand_shuffle[val_split:],:,:]\n",
    "\n",
    "\n",
    "if TRACK_CLASS:\n",
    "    print(test_event_indices.shape, val_data.shape, train_data.shape)\n",
    "    np.save('data/{}_4-track_testevent_indices'.format(ISOTOPE), test_event_indices)\n",
    "else:\n",
    "    print(test_data.shape, val_data.shape, train_data.shape)\n",
    "    np.save('data/' + name + 'test', test_data)\n",
    "np.save('data/' + name + 'train', train_data)\n",
    "np.save('data/' + name + 'val', val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac45d531",
   "metadata": {},
   "source": [
    "# Making Test Sets\n",
    "#### We make a pair of test sets to span all the data points, ensuring that each point is tested. This is useful for testing events that have been downsampled.\n",
    "*This is only necessary for semantic segmentation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70238be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making an array of the 4-track test event lengths\n",
    "if TRACK_CLASS:\n",
    "    name = ISOTOPE + '_4-track'\n",
    "    ev_indices = np.load('data/' + name + '_testevent_indices.npy')\n",
    "    num_events = ev_indices.shape[0]\n",
    "    test_ev_lens = np.zeros(num_events,int)\n",
    "    for i in tqdm.tqdm(range(num_events)):\n",
    "        event_ind = int(ev_indices[i,0,0])\n",
    "        test_ev_lens[i] = event_lens[event_ind]\n",
    "    np.save('data/' + name + '_testevent_lengths', test_ev_lens)\n",
    "    print(np.max(test_ev_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3324e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making an array of all the 4-track test data\n",
    "if TRACK_CLASS:\n",
    "    data = np.load('data/' + ISOTOPE + '_w_key_index.npy')\n",
    "    num_events = ev_indices.shape[0]\n",
    "    test_data = np.zeros((num_events,np.max(test_ev_lens),14),float)\n",
    "    instant = 0\n",
    "    for i in tqdm.tqdm(range(num_events)):\n",
    "        event_ind = int(ev_indices[i,0,0])\n",
    "        test_data[instant,:,:-1] = data[event_ind,:np.max(test_ev_lens),:]\n",
    "        test_data[instant,0,-1] = ev_indices[i,0,1]\n",
    "        test_data[instant,:,5] -= 1   #scaling the labels so they start at 0\n",
    "        instant += 1\n",
    "    np.save('data/' + name + '_testevents', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1084e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a normal sampled test dataset\n",
    "if TRACK_CLASS:\n",
    "    name = ISOTOPE + '_4-track'\n",
    "    test_events = np.load('data/' + name + '_testevent_indices.npy')\n",
    "    num_events = test_data.shape[0]\n",
    "    test_sample = np.zeros((num_events,sample_size,14),float)\n",
    "    incl_points = np.zeros((num_events,sample_size),int)\n",
    "    for i in range(num_events):\n",
    "        ev_len = test_ev_lens[i]    #accessing the event index to find event length\n",
    "        particle_ids = test_data[i,:ev_len,5]\n",
    "        label, distr = np.unique(particle_ids, return_counts=True)\n",
    "        shortest = label[np.argmin(distr)]\n",
    "        shortest_ind = np.argwhere(particle_ids == shortest)\n",
    "        if ev_len == sample_size:    #if array is already preferred length\n",
    "            test_sample[i,:,:] = test_data[i,:ev_len,:]\n",
    "            incl_points[i,:] = range(sample_size)\n",
    "        else:\n",
    "            instant = 0\n",
    "            for n in range(shortest_ind.size):    #the first instances sampled will be those belonging to the shortest track\n",
    "                test_sample[i,instant,:] = test_data[i,shortest_ind[n],:]\n",
    "                incl_points[i,instant] = shortest_ind[n]\n",
    "                instant += 1\n",
    "            need = sample_size - shortest_ind.size\n",
    "            random_points = np.random.choice(range(ev_len), need, replace= True if need > ev_len else False)  #choosing the random instances to sample\n",
    "            for r in random_points:\n",
    "                test_sample[i,instant,:] = test_data[i,r,:] \n",
    "                incl_points[i,instant] = r\n",
    "                instant += 1\n",
    "        test_sample[i,:,6] = incl_points[i,:]    #storing the indices in the original event as point IDs\n",
    "        test_sample[i,:,5] = list(map(alpha, test_sample[i,:,5]))    #making it BINARY\n",
    "    np.save('data/' + name + '_size{}test1'.format(sample_size), test_sample)\n",
    "\n",
    "    #array of the number of points not included in the first sample\n",
    "    not_incl = np.zeros(num_events, int)\n",
    "    for i in range(num_events):\n",
    "        incl = np.unique(incl_points[i])\n",
    "        not_incl[i] = event_lens[int(test_data[i,0,-2])] - incl.size\n",
    "    \n",
    "    #indices of the points not included in the first sample\n",
    "    not_incl_points = np.zeros((num_events,np.max(not_incl)),int)\n",
    "    for i in range(num_events):    #going through each event\n",
    "        count = 0\n",
    "        for p in range(event_lens[int(test_data[i,0,-2])]):    #going through each instant in the event\n",
    "            if p not in incl_points[i]:    #if that instant is not in the included points for that event\n",
    "                not_incl_points[i, count] = p\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62999147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a second test dataset that includes the points not in the first test dataset\n",
    "if TRACK_CLASS:\n",
    "    name = ISOTOPE + '_4-track'\n",
    "    test_data = np.load('data/' + name + '_testevents.npy')\n",
    "    num_events = test_data.shape[0]\n",
    "    test_sample = np.zeros((num_events,sample_size,14),float)\n",
    "    incl_points = np.zeros((num_events,sample_size),int)\n",
    "    for i in range(num_events):\n",
    "        ev_len = test_ev_lens[i]    #accessing the event index to find event length\n",
    "        particle_ids = test_data[i,:event_lens[i],5]\n",
    "        label, distr = np.unique(particle_ids, return_counts=True)\n",
    "        shortest = label[np.argmin(distr)]\n",
    "        shortest_ind = np.argwhere(particle_ids == shortest)\n",
    "        indices_sampled = np.zeros(sample_size, int)\n",
    "        if ev_len == sample_size:    #if array is already preferred length\n",
    "            test_sample[i,:,:] = test_data[i,:ev_len,:]\n",
    "            indices_sampled = range(sample_size)\n",
    "        else:\n",
    "            instant = 0\n",
    "            # including points not in the first set\n",
    "            for n in range(not_incl[i]):\n",
    "                test_sample[i,instant,:] = test_data[i,not_incl_points[i,n],:]\n",
    "                indices_sampled[instant] = not_incl_points[i,n]\n",
    "                instant += 1\n",
    "            if shortest_ind.size + not_incl[i] > sample_size:\n",
    "                #including as many points of shortest track as possible\n",
    "                for n in range(sample_size - not_incl[i]):    \n",
    "                    test_sample[i,instant,:] = test_data[i,shortest_ind[n],:]\n",
    "                    indices_sampled[instant] = shortest_ind[n]\n",
    "                    instant += 1\n",
    "            else:\n",
    "                #including all of shortest track\n",
    "                for n in range(shortest_ind.size):    \n",
    "                    test_sample[i,instant,:] = test_data[i,shortest_ind[n],:]\n",
    "                    indices_sampled[instant] = shortest_ind[n]\n",
    "                    instant += 1\n",
    "                random_points = np.random.choice(range(ev_len), sample_size - shortest_ind.size - not_incl[i])\n",
    "                #randomly sampling to get up to sample size\n",
    "                for r in random_points:\n",
    "                    test_sample[i,instant,:] = test_data[i,r,:]\n",
    "                    indices_sampled[instant] = r\n",
    "                    instant += 1\n",
    "        test_sample[i,:,6] = indices_sampled[:]    #storing the indices in the original event as point IDs\n",
    "        test_sample[i,:,5] = list(map(alpha, test_sample[i,:,5]))   \n",
    "    np.save('data/' + name + '_size{}test2'.format(sample_size), test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53447d40",
   "metadata": {},
   "source": [
    "### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bb0e3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3358\n",
      "(2015, 128, 14) (672, 128, 14) (671, 128, 14)\n",
      "6\n",
      "672\n",
      "671\n",
      "3358\n",
      "(2015, 128, 14) (672, 128, 14) (671, 128, 14)\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "if TRACK_CLASS:\n",
    "    name = 'data/' + ISOTOPE + '_4-track_size' + str(sample_size) + '{}.npy'\n",
    "    prev_data = np.load(name.format(''))\n",
    "    tr = np.load(name.format('train'))\n",
    "    va = np.load(name.format('val'))\n",
    "    te1 = np.load(name.format('test1'))\n",
    "    te2 = np.load(name.format('test2'))\n",
    "    print(len(prev_data))\n",
    "    print(tr.shape, va.shape, te1.shape, te2.shape)\n",
    "    print(len(np.unique(tr[:,:,5])))\n",
    "    print(len(np.unique(va[:,:,12])))\n",
    "    print(len(np.unique(te1[:,:,12])))\n",
    "    print(len(np.unique(te2[:,:,12])))\n",
    "else:\n",
    "    name = 'data/' + ISOTOPE + '_size' + str(sample_size) + '{}.npy'\n",
    "    prev_data = np.load(name.format('_sampled'))\n",
    "    tr = np.load(name.format('train'))\n",
    "    va = np.load(name.format('val'))\n",
    "    te = np.load(name.format('test'))\n",
    "    print(len(prev_data))\n",
    "    print(tr.shape, va.shape, te.shape)\n",
    "    print(len(np.unique(tr[:,:,5])))\n",
    "    print(len(np.unique(va[:,:,12])))\n",
    "    print(len(np.unique(te[:,:,12])))\n",
    "\n",
    "# works perfectly if length of dataset is even; if odd, rounding may be off\n",
    "# assert tr.shape == (np.ceil(len(prev_data) * .6) , sample_size, 14), 'Array has incorrect shape, check first for rounding error.'\n",
    "# assert va.shape == (np.ceil(len(prev_data) * .2) , sample_size, 14), 'Array has incorrect shape, check first for rounding error.'\n",
    "# assert te.shape == (np.ceil(len(prev_data) * .2) , sample_size, 14), 'Array has incorrect shape, check first for rounding error.'\n",
    "print(len(prev_data))\n",
    "if TRACK_CLASS:\n",
    "    print(tr.shape, va.shape, te1.shape, te2.shape)\n",
    "else:\n",
    "    print(tr.shape, va.shape, te.shape)\n",
    "print(len(np.unique(tr[:,:,5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3649d8af",
   "metadata": {},
   "source": [
    "# Rescaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "504d542f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(671, 128, 14) (2015, 128, 14) (672, 128, 14)\n"
     ]
    }
   ],
   "source": [
    "assert len(np.unique(np.isnan(tr[:,:,4]))) == 1, 'NaNs in dataset'\n",
    "assert len(np.unique(np.isnan(va[:,:,4]))) == 1, 'NaNs in dataset'\n",
    "if TRACK_CLASS:\n",
    "    assert len(np.unique(np.isnan(te1[:,:,4]))) == 1, 'NaNs in dataset'\n",
    "    assert len(np.unique(np.isnan(te2[:,:,4]))) == 1, 'NaNs in dataset'\n",
    "else:\n",
    "    assert len(np.unique(np.isnan(te[:,:,4]))) == 1, 'NaNs in dataset'\n",
    "assert np.any(tr[:,:,4]<0) == False, 'Dataset is incorrect, Negative charge values'\n",
    "assert np.any(va[:,:,4]<0) == False, 'Dataset is incorrect, Negative charge values'\n",
    "if TRACK_CLASS:\n",
    "    assert np.any(te1[:,:,4]<0) == False, 'Dataset is incorrect, Negative charge values'\n",
    "    assert np.any(te2[:,:,4]<0) == False, 'Dataset is incorrect, Negative charge values'\n",
    "else:\n",
    "    assert np.any(te[:,:,4]<0) == False, 'Dataset is incorrect, Negative charge values'\n",
    "\n",
    "scaled_val_data = copy.deepcopy(va)\n",
    "scaled_train_data = copy.deepcopy(tr)\n",
    "if TRACK_CLASS:\n",
    "    scaled_test_data1 = copy.deepcopy(te1)\n",
    "    scaled_test_data2 = copy.deepcopy(te2)\n",
    "else:\n",
    "    scaled_test_data = copy.deepcopy(te)\n",
    "# checking shapes\n",
    "if TRACK_CLASS:\n",
    "    print(scaled_test_data1.shape, scaled_test_data2.shape, scaled_train_data.shape, scaled_val_data.shape)\n",
    "else:\n",
    "    print(scaled_test_data.shape, scaled_train_data.shape, scaled_val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62161a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log scale all charges to reduce large range of values\n",
    "if TRACK_CLASS:\n",
    "    scaled_test_data1[:,:,4] = np.log10(te1[:,:,4] + 1e-10)\n",
    "    scaled_test_data2[:,:,4] = np.log10(te2[:,:,4] + 1e-10)\n",
    "else:\n",
    "    scaled_test_data[:,:,4] = np.log10(te[:,:,4] + 1e-10)\n",
    "scaled_train_data[:,:,4] = np.log10(tr[:,:,4] + 1e-10)\n",
    "scaled_val_data[:,:,4] = np.log10(va[:,:,4] + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d92eb158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# values correspond to the x,y,z,charge indices\n",
    "values = [0,1,2,4] \n",
    "means_and_stds = []\n",
    "# standard scaling \n",
    "for n in values:\n",
    "    mean = np.mean(scaled_train_data[:,:,n])\n",
    "    std = np.std(scaled_train_data[:,:,n])\n",
    "    means_and_stds.append([mean,std])\n",
    "    scaled_train_data[:,:,n] = (scaled_train_data[:,:,n] - mean) / std\n",
    "    scaled_val_data[:,:,n] = (scaled_val_data[:,:,n] - mean) / std\n",
    "    if TRACK_CLASS:\n",
    "        scaled_test_data1[:,:,n] = (scaled_test_data1[:,:,n] - mean) / std\n",
    "        scaled_test_data2[:,:,n] = (scaled_test_data2[:,:,n] - mean) / std\n",
    "    else:\n",
    "        scaled_test_data[:,:,n] = (scaled_test_data[:,:,n] - mean) / std\n",
    "\n",
    "if TRACK_CLASS:\n",
    "    name = 'data/' + ISOTOPE + '_4-track_size' + str(sample_size) + 'scaled_{}'\n",
    "    np.save(name.format('test_data1'), scaled_test_data1)\n",
    "    np.save(name.format('test_data2'), scaled_test_data2)\n",
    "else:\n",
    "    name = 'data/' + ISOTOPE + '_size' + str(sample_size) + 'scaled_{}'\n",
    "    np.save(name.format('test_data'), scaled_test_data)\n",
    "    \n",
    "np.save(name.format('mean_and_std_data'), means_and_stds)    \n",
    "np.save(name.format('train_data'), scaled_train_data)\n",
    "np.save(name.format('val_data'), scaled_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcd87ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.sum(np.isnan(scaled_train_data)) == 0, 'NaNs in dataset'\n",
    "assert np.sum(np.isnan(scaled_val_data)) == 0, 'NaNs in dataset'\n",
    "if TRACK_CLASS:\n",
    "    assert np.sum(np.isnan(scaled_test_data1)) == 0, 'NaNs in dataset'\n",
    "    assert np.sum(np.isnan(scaled_test_data2)) == 0, 'NaNs in dataset'\n",
    "else:\n",
    "    assert np.sum(np.isnan(scaled_test_data)) == 0, 'NaNs in dataset'\n",
    "    \n",
    "assert np.sum(np.isinf(scaled_train_data)) == 0, 'Infinities in dataset'\n",
    "assert np.sum(np.isinf(scaled_val_data)) == 0, 'Infinities in dataset'\n",
    "if TRACK_CLASS:\n",
    "    assert np.sum(np.isinf(scaled_test_data1)) == 0, 'Infinities in dataset'\n",
    "    assert np.sum(np.isinf(scaled_test_data2)) == 0, 'Infinities in dataset'\n",
    "else:\n",
    "    assert np.sum(np.isinf(scaled_test_data)) == 0, 'Infinities in dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc83bcf",
   "metadata": {},
   "source": [
    "# Get User Desired Inputs and make condensed array \n",
    "#### including x/y/z/c values, and then particle ID, event index, and number of tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67599262",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = np.zeros((len(scaled_train_data), sample_size, len(user_input)), float)\n",
    "new_val = np.zeros((len(scaled_val_data), sample_size, len(user_input)), float)\n",
    "if TRACK_CLASS:\n",
    "    new_test1 = np.zeros((len(scaled_test_data1), sample_size, len(user_input) + 1), float)\n",
    "    new_test2 = np.zeros((len(scaled_test_data2), sample_size, len(user_input) + 1), float)\n",
    "else:\n",
    "    new_test = np.zeros((len(scaled_test_data), sample_size, len(user_input) + 1), float)\n",
    "\n",
    "\n",
    "for i,index in enumerate(user_input):\n",
    "    new_train[:,:,i] = scaled_train_data[:,:,index]\n",
    "    new_val[:,:,i] = scaled_val_data[:,:,index]\n",
    "    if TRACK_CLASS:\n",
    "        new_test1[:,:,i] = scaled_test_data1[:,:,index]\n",
    "        new_test2[:,:,i] = scaled_test_data2[:,:,index]\n",
    "    else:\n",
    "        new_test[:,:,i] = scaled_test_data[:,:,index]\n",
    "        \n",
    "if TRACK_CLASS:\n",
    "    new_test1[:,:,-1] = scaled_test_data1[:,:,6]    #saving the POINT IDs (different from particle IDs)\n",
    "    new_test2[:,:,-1] = scaled_test_data2[:,:,6]\n",
    "else:\n",
    "    new_test[:,:,-1] = scaled_test_data[:,:,6]    #saving the POINT IDs (different from particle IDs)\n",
    "\n",
    "if TRACK_CLASS:\n",
    "    name = 'data/' + ISOTOPE + '_4-track_size' + str(sample_size) + '_convert' + PROJECTION + '_{}'\n",
    "else:\n",
    "    name = 'data/' + ISOTOPE + '_size' + str(sample_size) + '_convert' + PROJECTION + '_{}'\n",
    "\n",
    "np.save(name.format('train'), new_train)\n",
    "np.save(name.format('val'), new_val)\n",
    "\n",
    "if TRACK_CLASS:\n",
    "    np.save(name.format('test1'), new_test1)\n",
    "    np.save(name.format('test2'), new_test2)\n",
    "else:\n",
    "    np.save(name.format('test'), new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c368793a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2015, 128, 7)\n",
      "(672, 128, 7)\n",
      "(671, 128, 8)\n",
      "[ 4.27480579e-01 -8.01962178e-01  8.32781631e-01  3.02437847e-01\n",
      "  4.00000000e+00  5.69400000e+03  2.00000000e+00]\n",
      "[ 6.15746396e-01 -1.82605257e+00 -3.85121370e-01  1.89267296e+00\n",
      "  1.00000000e+00  9.68600000e+03  3.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# Checking shape after creating\n",
    "print(new_train.shape)\n",
    "print(new_val.shape)\n",
    "if TRACK_CLASS:\n",
    "    print(new_train[0,0,:])\n",
    "    print(new_test1.shape)\n",
    "    print(new_test2.shape)\n",
    "    \n",
    "else:\n",
    "    print(new_test.shape)\n",
    "print(new_val[0,0])\n",
    "print(new_train[0,0])\n",
    "\n",
    "assert np.sum(np.isnan(new_train)) == 0, 'NaNs in dataset'\n",
    "assert np.sum(np.isnan(new_val)) == 0, 'NaNs in dataset'\n",
    "if TRACK_CLASS:\n",
    "    assert np.sum(np.isnan(new_test1)) == 0, 'NaNs in dataset'\n",
    "    assert np.sum(np.isnan(new_test2)) == 0, 'NaNs in dataset'\n",
    "else:\n",
    "    assert np.sum(np.isnan(new_test)) == 0, 'NaNs in dataset'\n",
    "    \n",
    "assert np.sum(np.isinf(new_train)) == 0, 'Infinities in dataset'\n",
    "assert np.sum(np.isinf(new_val)) == 0, 'Infinities in dataset'\n",
    "if TRACK_CLASS:\n",
    "    assert np.sum(np.isinf(new_test1)) == 0, 'Infinities in dataset'\n",
    "    assert np.sum(np.isinf(new_test2)) == 0, 'Infinities in dataset'\n",
    "else:\n",
    "    assert np.sum(np.isinf(new_test)) == 0, 'Infinities in dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b80f773",
   "metadata": {},
   "source": [
    "# Make Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "209eda2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make histogram of values (either x,y,z,charge) from selected npy (train, val, test)\n",
    "# Will need to change PLOT and DATA_SET_NAME to plot X-Y-Z-Q(charge) from training, val, or test\n",
    "PLOT = 'Z'\n",
    "DATA_SET_NAME = '_train'\n",
    "index = PROJECTION.find(PLOT)\n",
    "\n",
    "if index != -1:\n",
    "    if TRACK_CLASS:\n",
    "        data = np.load('data/' + ISOTOPE + '_4-track_size' + str(sample_size) + '_convert' + PROJECTION + DATA_SET_NAME + '.npy')\n",
    "    else:\n",
    "        data = np.load('data/' + ISOTOPE + '_size' + str(sample_size) + '_convert' + PROJECTION + DATA_SET_NAME + '.npy')\n",
    "    info = data[:,:,index].flatten()\n",
    "    plt.hist(info, density=True, bins=100)\n",
    "    plt.ylabel('probibility')\n",
    "    plt.xlabel('value')\n",
    "    plt.title(PLOT + ' Distributions')\n",
    "    plt.show()\n",
    "    # plt.savefig('data/'+ '.png', bbox_inches = 'tight') # uncomment to save\n",
    "else:\n",
    "    print('Value to plot is invalid, change PLOT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ef4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphatodamoon",
   "language": "python",
   "name": "alphatodamoon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
